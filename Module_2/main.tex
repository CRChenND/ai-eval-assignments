\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{AI eval M2 assignment}
\author{Chaoran Chen}
\date{February 2026}

\begin{document}

\maketitle

\section{Introduction}

Large language models (LLMs) are increasingly deployed in high-stakes decision-making and engineering planning tasks. Despite strong reasoning and language capabilities, LLMs remain susceptible to hallucinations—confidently generated but factually incorrect or scientifically implausible outputs. Hallucinations become particularly problematic in multi-stage, cross-domain engineering problems, where errors may propagate across interdependent constraints and produce technically infeasible system designs.

This assignment investigates how different inference strategies influence hallucination rates and factual reliability in LLM-generated engineering plans. Specifically, we evaluate model performance under a hypothetical yet scientifically constrained deployment scenario: operating an 8-GPU large language model server on the Martian surface. The scenario intentionally integrates multiple technical domains, including thermodynamics, power systems engineering, orbital logistics, and distributed software deployment, thereby creating a realistic stress test for model reliability.

Rather than evaluating solution correctness alone, this study focuses on auditing model reasoning failures across inference strategies. We compare five distinct inference approaches, including zero-shot prompting, chain-of-thought reasoning, prompt iteration, reasoning-specialized models, and tool-augmented agentic inference. We then conduct a systematic factual audit by comparing generated outputs against established scientific and engineering constraints.

The contributions of this study are threefold:

\begin{itemize}
\item A structured evaluation framework for measuring hallucinations in multi-domain engineering planning tasks.
\item A comparative analysis of inference strategies and their impact on factual integrity.
\item A scientifically grounded deployment plan derived from the most reliable inference strategy.
\end{itemize}


\section{Deployment Scenario and Task Definition}

We consider a hypothetical deployment scenario in which a large language model must be operated locally on a high-performance computing system located on the Martian equatorial surface. The system consists of an 8-GPU NVIDIA server installed in a standard rack configuration.

Participants are tasked with generating a technical deployment plan that addresses the following stages:

\begin{itemize}
\item Transportation and landing of the hardware infrastructure on Mars.
\item Power system design capable of sustaining continuous server operation.
\item Thermal management and environmental adaptation within the Martian atmosphere.
\item Software deployment and operational orchestration of the LLM infrastructure.
\end{itemize}

This scenario requires coordination across several engineering domains, including power electronics, thermal physics, planetary environmental science, and distributed computing systems. The complexity of cross-domain constraints makes this task particularly suitable for evaluating hallucination behaviors in LLM-generated outputs.

The primary objective is not to produce a perfect engineering solution but to evaluate the reliability of model-generated reasoning under different inference strategies.


\section{Inference Strategy Comparison}

To evaluate the impact of inference design on model reliability, we compare five inference strategies that represent distinct prompting and reasoning paradigms.

\subsection{Zero-Shot Prompting}

Zero-shot prompting involves directly querying the model with the deployment task without providing structured reasoning guidance. This baseline captures default model behavior under minimal prompt engineering.

\subsection{Chain-of-Thought Reasoning}

Chain-of-thought (CoT) prompting encourages the model to explicitly decompose the problem into intermediate reasoning steps. Prior work suggests that CoT prompting can improve logical consistency in complex problem-solving tasks.

\subsection{Prompt Iteration}

Prompt iteration involves repeated or refined prompt queries to assess the stability and variance of model outputs. This approach evaluates whether hallucination patterns persist across multiple generations.

\subsection{Reasoning-Optimized Models}

We evaluate specialized reasoning-oriented model variants designed for complex analytical tasks. These models often incorporate architectural or training modifications intended to enhance multi-step reasoning.

\subsection{Agentic Tool-Augmented Inference}

Agentic inference integrates external knowledge retrieval or tool-calling mechanisms. This approach allows the model to verify environmental data, such as Martian solar irradiance or atmospheric conditions, thereby potentially reducing hallucinations.

\section{Evaluation Methodology}

\subsection{Factuality Auditing}

We perform a systematic factual audit by cross-referencing model-generated technical claims with established scientific datasets and engineering references. Evaluation focuses on three primary constraint categories:

\textbf{Power Feasibility:} Assessment of whether proposed energy sources can realistically sustain 8-GPU server operation under Martian environmental conditions.

\textbf{Environmental Physics:} Evaluation of thermal management strategies and their compatibility with Martian atmospheric properties.

\textbf{Deployment Validity:} Verification of software deployment workflows and system orchestration feasibility.

\subsection{Hallucination Detection}

Hallucinations are identified when generated outputs contain scientifically unsupported, physically implausible, or internally inconsistent claims.

\subsection{Claim Extraction and Fact-Check Protocol}

To make claim counting reproducible, we used a fully rule-based protocol (no LLM self-extraction in this stage):

\begin{itemize}
\item \textbf{Input source}: raw model outputs in \texttt{results/raw/*.json}.
\item \textbf{Sentence segmentation}: split text with regex \texttt{(?<=[.!?])\textbackslash s+}.
\item \textbf{Claim boundary}: one segmented sentence is treated as one candidate claim.
\item \textbf{Claim typing}: keyword rules assign each claim to \texttt{power}, \texttt{thermal}, \texttt{deployment}, or \texttt{other}.
\item \textbf{Numeric extraction}: regex \texttt{(-?\textbackslash d+(?:\textbackslash .\textbackslash d+)?)\textbackslash s*(kW|W|MW|C|°C|Pa|kPa|W/m2|W/m\^{}2|\%)?} extracts values and units.
\item \textbf{Filtering}: if a sentence has no numeric value and type=\texttt{other}, it is dropped; otherwise it is kept as a claim.
\item \textbf{Reference mapping}: each kept claim is mapped to one reference key in \texttt{mars\_reference.json} by keyword matching (e.g., solar/insolation, pressure, temperature, server power).
\item \textbf{Unit normalization}: \texttt{W}$\rightarrow$\texttt{kW}, \texttt{MW}$\rightarrow$\texttt{kW}, \texttt{kPa}$\rightarrow$\texttt{Pa}; other units are left unchanged.
\item \textbf{Verdict rule}: for mapped claims with numeric values, if any normalized value is outside the mapped reference range, verdict=\texttt{hallucinated}; if all are inside, verdict=\texttt{supported}; if no mapping or no numeric value, verdict=\texttt{unknown}.
\end{itemize}

Metrics are computed only on known claims:
\[
\text{Hallucination Rate}=\frac{N_{\text{hallucinated}}}{N_{\text{supported}}+N_{\text{hallucinated}}}.
\]

\subsection{Quantitative Metrics}

We evaluate inference strategies using:

\begin{itemize}
\item Hallucination rate (percentage of incorrect technical claims)
\item Constraint violation count
\item Cross-run consistency score
\item Cross-run output variance
\end{itemize}

In this implementation, consistency is defined from repeated runs of the same strategy:
\[
r_i=\frac{N^{(i)}_{\text{hallucinated}}}{N^{(i)}_{\text{supported}}+N^{(i)}_{\text{hallucinated}}},
\quad
\sigma_r=\mathrm{stdev}(r_1,\dots,r_k),
\]
\[
\text{Consistency}=\max(0,\min(1,1-\sigma_r)).
\]
Here, \(r_i\) is run-level hallucination rate for run \(i\), and \(k\) is the number of non-empty runs for that strategy. Variance reported in this report corresponds to \(\sigma_r\). For single-run strategies (\(k=1\)), \(\sigma_r=0\) by definition, so Consistency \(=1.000\); therefore, this metric is most informative for multi-run settings (e.g., prompt iteration).

\section{Hallucination Taxonomy}

To systematically characterize reasoning failures, we categorize hallucinations into four types:

\textbf{Scientific Hallucinations:} Violations of known physical or engineering laws.

\textbf{Numerical Hallucinations:} Incorrect quantitative estimates such as power consumption or thermal load.

\textbf{Procedural Hallucinations:} Invalid or incomplete deployment workflows.

\textbf{Cross-Domain Inconsistencies:} Conflicts between constraints across engineering domains.

\section{Experimental Setup}

All experiments were implemented in a unified Python pipeline and executed with OpenRouter API access. The evaluation prompt required structured outputs with assumptions, deployment steps, quantitative estimates, risk analysis, and a final feasibility verdict. Five strategies were configured:

\begin{itemize}
\item Zero-shot prompting
\item Chain-of-thought (CoT) prompting
\item Prompt iteration (5 runs configured)
\item Reasoning-focused model variant
\item Tool-augmented prompting with injected Mars reference facts
\end{itemize}

The default model family used was Qwen (\texttt{qwen/qwen3-32b}) with a reasoning variant (\texttt{qwen/qwq-32b}) for the reasoning-focused condition. Claims were automatically extracted and checked against \texttt{mars\_reference.json} constraints.

\section{Results}

\subsection{Generation Success and Coverage}

Table~\ref{tab:coverage} summarizes raw generation coverage.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|l|}
\hline
Strategy & Configured Runs & Non-empty Runs & Notes \\
\hline
Zero-shot & 1 & 1 & Full text captured (finish: stop) \\
Chain-of-thought & 1 & 1 & Partial text captured (finish: length) \\
Prompt iteration & 5 & 3 & 2 runs truncated to empty output \\
Reasoning model & 1 & 1 & Complete output captured \\
Tool-augmented & 1 & 1 & Partial output captured (length-limited) \\
\hline
\end{tabular}
\caption{Observed generation coverage from \texttt{results/raw/*.json}.}
\label{tab:coverage}
\end{table}

\subsection{Hallucination Metrics}

All five strategies produced measurable claims in this rerun. The computed metrics are shown in Table~\ref{tab:metrics}.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
Strategy & Known Claims & Hallucinated Claims & Hallucination Rate & Consistency \\
\hline
Zero-shot & 23 & 23 & 1.000 & 1.000 \\
Chain-of-thought & 21 & 20 & 0.952 & 1.000 \\
Prompt iteration & 71 & 67 & 0.944 & 0.958 \\
Reasoning model & 22 & 22 & 1.000 & 1.000 \\
Tool-augmented & 8 & 7 & 0.875 & 1.000 \\
\hline
\end{tabular}
\caption{Strategy-level factuality metrics from \texttt{results/analysis/metrics.json}.}
\label{tab:metrics}
\end{table}

Among evaluated strategies, the tool-augmented setup achieved the lowest measured hallucination rate (0.875). The highest rate (1.000) was observed in both zero-shot and reasoning-model runs. Consistency values of 1.000 for single-run strategies should be interpreted as a metric artifact (no cross-run dispersion available), not as proof of internal logical coherence.

\subsection{Categorized Hallucination Log}

Detected hallucinations were grouped by claim category (Table~\ref{tab:hallcat}).

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Strategy & Total Hallucinations & Power & Thermal & Deployment & Other \\
\hline
Zero-shot & 23 & 12 & 7 & 3 & 1 \\
Chain-of-thought & 20 & 13 & 3 & 1 & 3 \\
Prompt iteration & 67 & 34 & 13 & 12 & 8 \\
Reasoning model & 22 & 13 & 5 & 2 & 2 \\
Tool-augmented & 7 & 1 & 3 & 2 & 1 \\
\hline
\end{tabular}
\caption{Categorized hallucination counts from \texttt{results/fact\_check/fact\_check.json}.}
\label{tab:hallcat}
\end{table}

Power-related numerical claims were the dominant failure mode in zero-shot, chain-of-thought, prompt-iteration, and reasoning-focused outputs.

\subsection{Qualitative Error Analysis (Empirical Cases)}

To complement aggregate metrics, we include representative hallucination cases from the rule-based audit. To reduce cherry-picking risk, cases were selected with a fixed protocol: (i) keep \texttt{verdict=hallucinated} entries from \texttt{fact\_check.json}, (ii) retain semantically aligned claim--reference pairs (e.g., surface insolation claims checked against surface insolation bounds), (iii) prioritize by operational risk (power sizing first), and (iv) report cross-strategy diversity.

\begin{table}[h]
\centering
\small
\begin{tabular}{|p{2.2cm}|p{4.3cm}|p{2.2cm}|p{3.2cm}|p{3.2cm}|}
\hline
Strategy & Claim excerpt (model output) & Reference bound & Why flagged as hallucination & Operational impact \\
\hline
Zero-shot & ``Martian equatorial solar irradiance = 550 W/m$^2$'' (surface context) & Surface noon insolation: 200--450 W/m$^2$ & Overestimates surface-available power for sizing assumptions & Undersized storage / overconfident continuous operation planning \\
\hline
Chain-of-thought & ``8 $\times$ H100 ... 2.4 kW total'' for full rack continuous operation & 8-GPU rack power: 2.5--6.5 kW & Below evaluated lower bound for full-system rack budget & Cascading underestimation of daily energy and radiator requirements \\
\hline
Prompt iteration & ``Solar irradiance: 590 W/m$^2$ (equatorial average)'' in surface power sizing & Surface noon insolation: 200--450 W/m$^2$ & Uses near-TOA value as surface planning input & Solar array area is under-sized, reducing dust-season reliability margin \\
\hline
Reasoning model & ``Solar irradiance: 590 W/m$^2$ at equator'' with solar augmentation sizing & Surface noon insolation: 200--450 W/m$^2$ & Same surface-resource inflation in quantitative design path & Overstates redundant generation capability during degraded conditions \\
\hline
Tool-augmented & ``Server components must operate between -20$^\circ$C and 40$^\circ$C'' used as environmental adequacy claim & Mars mean surface temp: -65 to -55$^\circ$C & Confuses hardware operating envelope with external climate adequacy & Can hide enclosure/heating requirements if interpreted as direct ambient compatibility \\
\hline
\end{tabular}
\caption{Representative empirical hallucination cases selected by a fixed audit protocol.}
\label{tab:qual_cases}
\end{table}

These cases provide concrete failure modes behind Table~\ref{tab:metrics}: most high-impact errors are power-budget hallucinations (resource inflation or load underestimation), with additional thermal framing errors that can mislead enclosure design and survivability analysis.

\subsection{Visual Comparison}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{results/plots/hallucination_bar_chart.png}
\caption{Hallucination rate by strategy.}
\label{fig:hall_bar}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{results/plots/strategy_comparison.png}
\caption{Consistency comparison across evaluated strategies.}
\label{fig:consistency_bar}
\end{figure}

\section{Optimal Deployment Plan (Derived from Best Strategy)}

The tool-augmented strategy was selected as the most reliable among successfully evaluated runs (lowest measured hallucination rate). The final recommended plan is therefore based on this strategy, but with a conservative feasibility decision:

\begin{itemize}
\item \textbf{Compute and load budget (design point):} continuous IT load \(=3.5\ \mathrm{kW}\) (within the evaluated \(2.5\) to \(6.5\ \mathrm{kW}\) reference band), yielding daily energy demand
\[
E_{\text{load}}=3.5\times 24.6\approx 86.1\ \mathrm{kWh/sol}.
\]
\item \textbf{Primary power path (scientifically viable baseline):} nuclear-primary architecture with \(\ge 10\ \mathrm{kWe}\) continuous generation margin for IT load + thermal auxiliaries + degradation reserve. Solar-only architecture is treated as non-baseline because dust-season reliability is weak for continuous 24.6 h operation.
\item \textbf{Secondary power path (if solar is used):} with equatorial surface insolation \(200\) to \(450\ \mathrm{W/m^2}\), panel efficiency \(30\%\), and system derating factor \(0.7\), effective electrical yield is about \(42\) to \(95\ \mathrm{W/m^2}\) during daylight. For \(86.1\ \mathrm{kWh/sol}\), required array size is on the order of \(90\) to \(200\ \mathrm{m^2}\), plus storage.
\item \textbf{Energy storage requirement:} minimum \(12\ \mathrm{h}\) autonomy at full load requires
\[
E_{\text{batt,min}}=3.5\times 12=42\ \mathrm{kWh},
\]
and \(24\ \mathrm{h}\) autonomy requires \(84\ \mathrm{kWh}\). Mission design target should be \(\ge 84\ \mathrm{kWh}\) usable storage after degradation.
\item \textbf{Thermal control requirement:} reject approximately \(3.5\ \mathrm{kW}\) waste heat under low-pressure Mars atmosphere using radiator-dominant heat rejection; enclosure temperature control band should remain within IT hardware limits (target internal \(0^\circ\mathrm{C}\) to \(35^\circ\mathrm{C}\)) with active monitoring and staged throttling.
\item \textbf{Operations concept:} pre-integrated software image, autonomous fault management, and power-aware scheduling with three modes: \textit{Nominal} (\(100\%\) load), \textit{Derated} (\(50\%\) load), \textit{Survival} (\(<10\%\) load, critical services only).
\item \textbf{Go/No-Go gates before launch:}
\begin{itemize}
\item Demonstrated \(>20\%\) end-to-end power margin at worst-case insolation/dust assumptions.
\item Demonstrated thermal stability for 72 h in Mars-environment chamber at full-load heat rejection.
\item Demonstrated autonomous transition and recovery across Nominal/Derated/Survival modes.
\end{itemize}
\item \textbf{Verdict:} \textbf{Conditional Go}, contingent on closing the power-reliability gap with nuclear-primary supply (or equivalent continuous source) and passing the above qualification gates.
\end{itemize}

\section{Discussion and Limitations}

The experimental pipeline successfully demonstrates strategy-dependent variation in factual reliability, but three limitations affected this run:

\begin{itemize}
\item \textbf{Output truncation}: chain-of-thought, tool-augmented, and several iterative runs still hit length limits.
\item \textbf{Claim-mapping noise}: automated claim-to-reference matching is keyword-based and can over-penalize valid statements.
\item \textbf{Claim boundary sensitivity}: one sentence is treated as one claim, so different punctuation or formatting can change claim counts and rates.
\item \textbf{Single-run variance for most strategies}: only prompt iteration had multiple non-empty runs, limiting robust variance comparison across all methods.
\end{itemize}

Even with these limitations, the observed trend suggests that adding explicit reference grounding improves factual robustness relative to unconstrained reasoning outputs.

\section{Conclusion}

This study evaluated five inference strategies for Mars-side 8-GPU deployment planning and audited hallucinations through structured fact-checking. In this rerun, all five strategies yielded analyzable claims; tool-augmented prompting still produced the best factuality, while zero-shot and reasoning-model outputs showed the highest hallucination rates. Across strategies, numerical power claims remained the most frequent failure mode. The selected optimal plan is therefore a \textit{conditional} deployment plan grounded in tool-supported evidence and conservative engineering constraints.

\end{document}
